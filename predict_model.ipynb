{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "random_state = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the transformers\n",
    "class VariableExtractor(TransformerMixin):\n",
    "    '''Extract variable(s).'''    \n",
    "    def _init_(self, variables):\n",
    "        self.variables = variables\n",
    "        \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset[self.features]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RatioCreator(TransformerMixin):\n",
    "    '''Create new variable as the ratio of two variables.'''\n",
    "    def _init_(self, variable1, variable2):\n",
    "        self.variable1 = variable1\n",
    "        self.variable2 = variable2\n",
    "        \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(dataset):\n",
    "        return dataset[variable1]/dataset[variable2].apply(lambda x: x if x != 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DimOneUp(TransformerMixin):\n",
    "    '''Turn Series into array with 2 dimensions'''\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, series):\n",
    "        return series.reshape((series.shape[0], 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cluster longitutide and latitute\n",
    "class LLCluster(TransformerMixin):\n",
    "    '''Cluster longitude and latitude.'''\n",
    "    def _init_(self, n_cluster, *args):\n",
    "        self.model = MiniBatchKMeans(n_cluster=n_cluster, *args)\n",
    "        \n",
    "    def fit(self, dataset):\n",
    "        self.model.fit(dataset)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return self.model.predict(dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariableLength(TransformerMixin):\n",
    "    '''Get the length of the variable when it is a list.'''\n",
    "    def _init_(self, variable):\n",
    "        self.feature = variable\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset[variable].apply(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureCleanser(TransformerMixin):\n",
    "    '''Clean the features\n",
    "       Typical features in the data set: ['featureA', 'featureB']\n",
    "       But some features are like ['featureA**featureB'].\n",
    "       Turn those features into ['featureA', 'featureB']\n",
    "    '''\n",
    "    \n",
    "    def _init_(self, spliter=['*', '.', '^']):\n",
    "        self.spliter = spliter\n",
    "    \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset['features'].apply(self.feature_clean)\n",
    "            \n",
    "    def feature_clean(self, feature_list):\n",
    "        '''Clean the features.'''\n",
    "        for ff in feature_list: \n",
    "            if any(x in ff for x in self.spliter):\n",
    "                feature_list.remove(ff)\n",
    "                ff = re.sub('[{}]+'.format('|'.join(self.spliter)), ',', ff)\n",
    "                #ff = re.sub('[*|.|^]+', ',', ff)\n",
    "                # remove the ',' at the beginning and at the end of the string\n",
    "                ff = re.sub('^[,]|[,]$', '', ff)\n",
    "                feature_list += ff.split(',')\n",
    "        # clean the text, strip and lower case\n",
    "        return [f.strip().lower() for f in feature_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DiffFeatCounts(TransformerMixin):\n",
    "    '''For the Feature record, create a data set to count the most different features across classes.\n",
    "    '''\n",
    "    def _init_(self, sample_size, min_freq, n_iter=10, threshold=0.5, random_state=0):\n",
    "        self.sample_size = sample_size\n",
    "        self.min_freq = min_freq\n",
    "        self.n_iter = n_iter\n",
    "        self.threshold = threshold\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, dataset, y, *args):\n",
    "        self.fit_set = dataset\n",
    "        self.y = y\n",
    "        self.diff_feat = self.find_DiffFeat()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset['features'].features.apply(self.feature_counts)\n",
    "\n",
    "    def feature_counts(self, features):\n",
    "        '''For each 'features' record, count the frequency of the different features in the feature record and\n",
    "           create the data frame based on the counts. \n",
    "        '''\n",
    "        feat_series = pd.Series([0]*length(self.diff_feat), index=self.difffeat_list)\n",
    "        for f in self.diff_feat:\n",
    "            feat_series[f] = features.count(f)\n",
    "        return feat_series\n",
    "    \n",
    "    def find_DiffFeat(self):\n",
    "        '''Find the most different features across the interest levels.\n",
    "           Criteria: features appear > min_freq\n",
    "                     any(%interest_level > threshold)\n",
    "           Return a list of different features.                      \n",
    "        '''\n",
    "        random.seed(self.random_state)\n",
    "        feature_df = default_dict(default_dict(0))\n",
    "        # feature_df = {'featureA': {'low':30, 'medium':10, 'high':2}, 'featureB': ...}\n",
    "        data_addy = self.fit_set.copy()\n",
    "        data_addy['interest_level'] = self.y\n",
    "        # Iterate the process. In each iteration, sample a subset with equal number of each interest level\n",
    "        for n in range(self.n_iter):\n",
    "            data_temp = pd.DataFrame(columns=['features', 'interest_level'])\n",
    "            # for each interest level, sample equal size \n",
    "            for i in self.y.unique():\n",
    "                data_temp = data_temp.append(data_addy[self.y==i].sample(n=self.sample_size))\n",
    "            for ind in data_temp.index:\n",
    "                for f in data_temp.loc[ind, 'features']:\n",
    "                    feature_df[f][data_temp.loc[ind, 'interest_level']] += 1\n",
    "                    \n",
    "        diff_feat = [fk for fk, fv in feature_df.iter() if sum(fv.values()) >= self.min_freq \n",
    "                     and max(fv.values())/sum(fv.values()) >= self.threshold\n",
    "                    ]\n",
    "        return diff_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DescriptionProcessor(TransformerMixin):\n",
    "    '''Process the description.'''\n",
    "    def _init_(self, *args):\n",
    "        self.stemmer = snowball('english')\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+') #only keep the words, do not keep the punctuations\n",
    "        self.vectorizer = TfidfVectorizer(*args, preprocessor=lambda p: self.preprocessor(p, stemmer=stemmer, tokenizer=tokenizer))\n",
    "        \n",
    "    def fit(self, description):\n",
    "        self.vectorizer.fit(description)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, description):\n",
    "        return self.vectorizer.transform(description).toarray()\n",
    "\n",
    "    def preprocessor(self, text, stemmer, tokenizer):\n",
    "        '''Preprocess the description'''\n",
    "        # remove numbers\n",
    "        text = re.sub('[0-9]*', '', text)\n",
    "        #toknize the description\n",
    "        text = ' '.join([self.stemmer.stem(x) for x in self.tokenizer.tokenize(text)])\n",
    "        return text\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CatFeatureCounts(TransformerMixin):\n",
    "    '''Count number of list for each categorical variable.'''       \n",
    "    def fit(self, dataseries):\n",
    "        self.catcounts = dataseries.value_counts\n",
    "        return self\n",
    "        \n",
    "    def transform(self, dataseries):\n",
    "        dataseries.apply(lambda x: self.catcounts[x] if x in self.catcounts.index else 0)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CatFeatureIndicator(TransformerMixin):\n",
    "    '''Indicate if the categorical variable has more low, medium or high.\n",
    "       Criteria: Frequency of the category > min_list\n",
    "                 For a category, the percent of any interest_level greater than the corresponding threshold. \n",
    "    '''\n",
    "    def _init_(self, min_list, threshold):\n",
    "        self.min_list = min_list\n",
    "        self.threshold = threshold\n",
    "        self.hml_features = defaultdit([])\n",
    "        \n",
    "    def fit(self, dataseries, y):\n",
    "        cat_counts = dataseries.value_counts()\n",
    "        self.ylevels = y.unique()\n",
    "        # restrict to records with listings more than the min_list\n",
    "        elig_data = data[data.isin(cat_counts[cat_counts>=self.min_list].index.values)]\n",
    "        for d in elig_data.unique():\n",
    "            y_pectages = self.ypect(dataseries, y, d)\n",
    "            for ylevel in self.ylevels: \n",
    "                try:\n",
    "                    if y_pectages[ylevel] >= threshold:\n",
    "                        self.hml_features[ylevel].append(d)\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataseries):\n",
    "        return dataseries.apply(self.single_transform)\n",
    "        \n",
    "    def y_pect(self, dataseries, y, category):\n",
    "        return y[dataseries==category].count_values(normalize=True)\n",
    "    \n",
    "    def single_transform(self, category):\n",
    "        return np.array([(category in v) for v in self.hml_features.values])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DateProcessor(TransformMixin):\n",
    "    '''Returns the year, month and hour of the date'''\n",
    "    def _init_(self, wantyear=False, wantmonth=False, wanthour=True):\n",
    "        self.wantyear= wantyear\n",
    "        self.wantmonth = wantmonth\n",
    "        self.wanthour = wanthour\n",
    "        \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset['created'].apply(self).iloc[:, [self.wantyear, self.wantmonth, self.wanthour]]\n",
    "    \n",
    "    def process_date(self, date):\n",
    "        year = date[:4]\n",
    "        month = date[5:7]\n",
    "        hour = date[11:13]\n",
    "        return pd.Series([year, month, hour], index=('year', 'month', 'hour'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddressCleanser(TransformMixin):\n",
    "    '''Clean the address.\n",
    "        Strip and lowcase the address. Standardize synonyms into one expression. \n",
    "    '''\n",
    "    def _init_(self, synonyms):\n",
    "        self.synonyms = synonyms\n",
    "        \n",
    "    def fit(self):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataseries):\n",
    "        return dataseries.apply(self.clean_address)\n",
    "        \n",
    "    def clean_address(self, address):\n",
    "        address = address.strip()\n",
    "        address = address.lower()        \n",
    "        for s1, s2 in aself.synonyms:\n",
    "            address = re.sub(s1, s2, address)\n",
    "        return address\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in training data set with size of 49352 * 15\n",
      "Read in testing data set with size of 74659 * 14\n"
     ]
    }
   ],
   "source": [
    "# read in the data \n",
    "# read in the training data set\n",
    "train_set = pd.read_json(r'..\\data\\train.json')\n",
    "test_set = pd.read_json(r'..\\data\\test.json')\n",
    "print(\"Read in training data set with size of {} * {}\".format(train_set.shape[0], train_set.shape[1]))\n",
    "print(\"Read in testing data set with size of {} * {}\".format(test_set.shape[0], test_set.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
