{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "random_state = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the transformers\n",
    "class VariableExtractor(TransformerMixin):\n",
    "    '''Extract variable(s).'''    \n",
    "    def __init__(self, variables):\n",
    "        self.variables = variables\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset[self.variables]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RatioCreator(TransformerMixin):\n",
    "    '''Create new variable as the ratio of two variables.'''\n",
    "    def __init__(self, variable1, variable2):\n",
    "        self.variable1 = variable1\n",
    "        self.variable2 = variable2\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset[self.variable1]/dataset[self.variable2].apply(lambda x: x if x != 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DimOneUp(TransformerMixin):\n",
    "    '''Turn Series into array with 2 dimensions'''\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, series):\n",
    "        return series.reshape((series.shape[0], 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cluster longitutide and latitute\n",
    "class LLCluster(TransformerMixin):\n",
    "    '''Cluster longitude and latitude.'''\n",
    "    def __init__(self, n_clusters, **kargs):\n",
    "        self.model = MiniBatchKMeans(n_clusters=n_clusters, **kargs)\n",
    "        \n",
    "    def fit(self, dataset, *_):\n",
    "        self.model.fit(dataset)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return self.model.predict(dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VariableLength(TransformerMixin):\n",
    "    '''Get the length of the variable when it is a list.'''\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataseries):\n",
    "        return dataseries.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureCleanser(TransformerMixin):\n",
    "    '''Clean the features\n",
    "       Typical features in the data set: ['featureA', 'featureB']\n",
    "       But some features are like ['featureA**featureB'].\n",
    "       Turn those features into ['featureA', 'featureB']\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, spliter=['*', '.', '^']):\n",
    "        self.spliter = spliter\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset['features'].apply(self.feature_clean)\n",
    "            \n",
    "    def feature_clean(self, feature_list):\n",
    "        '''Clean the features.'''\n",
    "        for ff in feature_list: \n",
    "            if any(x in ff for x in self.spliter):\n",
    "                feature_list.remove(ff)\n",
    "                ff = re.sub('[{}]+'.format('|'.join(self.spliter)), ',', ff)\n",
    "                #ff = re.sub('[*|.|^]+', ',', ff)\n",
    "                # remove the ',' at the beginning and at the end of the string\n",
    "                ff = re.sub('^[,]|[,]$', '', ff)\n",
    "                feature_list += ff.split(',')\n",
    "        # clean the text, strip and lower case\n",
    "        return [f.strip().lower() for f in feature_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DiffFeatCounts(TransformerMixin):\n",
    "    '''For the Feature record, create a data set to count the most different features across classes.\n",
    "    '''\n",
    "    def __init__(self, sample_size=1000, min_freq=200, n_iter=10, threshold=0.5, random_state=0):\n",
    "        self.sample_size = sample_size\n",
    "        self.min_freq = min_freq\n",
    "        self.n_iter = n_iter\n",
    "        self.threshold = threshold\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, dataseries, y, *_):\n",
    "        self.fit_set = dataseries\n",
    "        self.y = y\n",
    "        self.diff_feat = self.find_DiffFeat()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataseries):\n",
    "        return dataseries.apply(self.feature_counts)\n",
    "\n",
    "    def feature_counts(self, features):\n",
    "        '''For each 'features' record, count the frequency of the different features in the feature record and\n",
    "           create the data frame based on the counts. \n",
    "        '''\n",
    "        feat_series = pd.Series([0]*len(self.diff_feat), index=self.diff_feat)\n",
    "        for f in self.diff_feat:\n",
    "            feat_series[f] = features.count(f)\n",
    "        return feat_series\n",
    "    \n",
    "    def find_DiffFeat(self):\n",
    "        '''Find the most different features across the interest levels.\n",
    "           Criteria: features appear > min_freq\n",
    "                     any(%interest_level > threshold)\n",
    "           Return a list of different features.                      \n",
    "        '''\n",
    "        random.seed(self.random_state)\n",
    "        feature_df = defaultdict(lambda: defaultdict(int))\n",
    "        # feature_df = defaultdict(defaultdict(int)), this did not work\n",
    "        # feature_df = {'featureA': {'low':30, 'medium':10, 'high':2}, 'featureB': ...}\n",
    "        data_addy = pd.concat([self.fit_set, self.y], axis=1)\n",
    "        # Iterate the process. In each iteration, sample a subset with equal number of each interest level\n",
    "        for n in range(self.n_iter):\n",
    "            data_temp = pd.DataFrame(columns=['features', 'interest_level'])\n",
    "            # for each interest level, sample equal size \n",
    "            for i in self.y.unique():\n",
    "                data_temp = data_temp.append(data_addy[self.y==i].sample(n=self.sample_size))\n",
    "            for ind in data_temp.index:\n",
    "                for f in data_temp.loc[ind, 'features']:\n",
    "                    feature_df[f][data_temp.loc[ind, 'interest_level']] += 1         \n",
    "        diff_feat = [fk for fk, fv in feature_df.items() if sum(fv.values()) >= self.min_freq \n",
    "                     and max(fv.values())/sum(fv.values()) >= self.threshold\n",
    "                    ]\n",
    "        return diff_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DescriptionWordCounts(TransformerMixin):\n",
    "    '''Count the number of words innthe description.'''\n",
    "    def __init__(self, tokenizer=RegexpTokenizer(r'\\w+')):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return len(self.tokenize(dataset['description']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DescriptionProcessor(TransformerMixin):\n",
    "    '''Process the description.'''\n",
    "    def __init__(self, stemmer=SnowballStemmer('english'), tokenizer=RegexpTokenizer(r'\\w+'), min_df=5000, stop_words='english', *args):\n",
    "        self.vectorizer = TfidfVectorizer(preprocessor=lambda p: self.preprocessor(p, stemmer=stemmer, tokenizer=tokenizer))\n",
    "        \n",
    "    def fit(self, dataset, *_):\n",
    "        self.vectorizer.fit(dataset['description'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return self.vectorizer.transform(dataset['description']).toarray()\n",
    "\n",
    "    def preprocessor(self, text, stemmer, tokenizer):\n",
    "        '''Preprocess the description.'''\n",
    "        # remove numbers\n",
    "        text = re.sub('[0-9]*', '', text)\n",
    "        #tokenize the description, stem each word and link words back into sentences\n",
    "        text = ' '.join([stemmer.stem(x) for x in tokenizer.tokenize(text)])\n",
    "        return text\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CatVariableCounts(TransformerMixin):\n",
    "    '''Count number of lists for each category of the categorical variable.\n",
    "       e.g. How many lists does a manager have    \n",
    "    '''       \n",
    "    \n",
    "    def fit(self, dataseries, *_):\n",
    "        self.catcounts = dataseries.value_counts()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, dataseries):\n",
    "        return dataseries.apply(lambda x: self.catcounts[x] if x in self.catcounts.index else 0)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CatVariableIndicator(TransformerMixin):\n",
    "    '''Find the category levels of the categorical variable which have more low or more medium or more high. \n",
    "       Criteria: Frequency of the category > min_list\n",
    "                 For a category, the percent of any interest_level greater than the corresponding threshold. \n",
    "    '''\n",
    "    def __init__(self, min_list=4, threshold={'low': 0.8, 'medium': 0.6, 'low': 0.4}):\n",
    "        '''\n",
    "            Args:\n",
    "                variable: name of the categorical variable\n",
    "                min_list: minimal number of the list the category should have\n",
    "                threshold: a dictionary haing the thresholds for each interest level (thresholds in percentage)\n",
    "        \n",
    "        '''\n",
    "        self.min_list = min_list\n",
    "        self.threshold = threshold\n",
    "        self.hml_features = defaultdict(list)\n",
    "        \n",
    "    def fit(self, dataseries, y, *_):\n",
    "        cat_counts = dataseries.value_counts()\n",
    "        self.ylevels = y.unique()\n",
    "        # restrict to records with listings more than the min_list\n",
    "        elig_data = dataseries[dataseries.isin(cat_counts[cat_counts>=self.min_list].index.values)]\n",
    "        elig_y = y[dataseries.isin(cat_counts[cat_counts>=self.min_list].index.values)]\n",
    "        for category in elig_data.unique():\n",
    "            y_pectages = self.y_pect(elig_data, elig_y, category)\n",
    "            for ylevel in self.ylevels: \n",
    "                try:\n",
    "                    if y_pectages[ylevel] >= self.threshold[ylevel]:\n",
    "                        self.hml_features[ylevel].append(category)\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataseries):\n",
    "        return dataseries.apply(self.single_transform)\n",
    "        \n",
    "    def y_pect(self, dataseries, y, category):\n",
    "        return y[dataseries==category].value_counts(normalize=True)\n",
    "    \n",
    "    def single_transform(self, category):\n",
    "        return pd.Series([(category in v) for v in self.hml_features.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DateProcessor(TransformerMixin):\n",
    "    '''Returns the year, month and hour of the date'''\n",
    "    def __init__(self, wantyear=False, wantmonth=False, wanthour=True):\n",
    "        self.wantyear= wantyear\n",
    "        self.wantmonth = wantmonth\n",
    "        self.wanthour = wanthour\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset['created'].apply(self).iloc[:, [self.wantyear, self.wantmonth, self.wanthour]]\n",
    "    \n",
    "    def process_date(self, date):\n",
    "        year = date[:4]\n",
    "        month = date[5:7]\n",
    "        hour = date[11:13]\n",
    "        return pd.Series([year, month, hour], index=('year', 'month', 'hour'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddressCleanser(TransformerMixin):\n",
    "    '''Clean the address.\n",
    "        Strip and lowcase the address. Standardize synonyms into one expression. \n",
    "    '''\n",
    "    def __init__(self, synonyms=[(r'([\\d])((st)|(nd)|(rd)|(th))', r'\\1'),(r'( street)|( st)', r' st.'), \n",
    "                                 (r'( avenue)|( ave)', r' ave.'), (r'(w )', r'west '),(r'(e )', r'east '), \n",
    "                                 (r'(n )', r'north '), (r'(s )', r'south ')], variable='display_address'):\n",
    "        self.synonyms = synonyms\n",
    "        self.variable = variable\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, dataset):\n",
    "        return dataset[self.variable].apply(self.clean_address)\n",
    "        \n",
    "    def clean_address(self, address):\n",
    "        address = address.strip()\n",
    "        address = address.lower()        \n",
    "        for s1, s2 in self.synonyms:\n",
    "            address = re.sub(s1, s2, address)\n",
    "        return address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_predprob(pipeline, test_X=test_X, tocsv=False, filename=None):\n",
    "    '''Function to make prediction probability matrix.'''\n",
    "    test_pred_prob = pipeline.predict_proba(test_X)\n",
    "    test_pred_prob_set = pd.DataFrame(test_pred_prob, columns=pipeline.classes_, index=test_set.index)\n",
    "    test_pred_prob_set = test_pred_prob_set.join(test_set['listing_id'])\n",
    "    col_orders = ['listing_id', 'high', 'medium', 'low']\n",
    "    test_pred_prob_set = test_pred_prob_set[col_orders]\n",
    "    if tocsv:\n",
    "        test_pred_prob_set.to_csv(filename, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in training data set with size of 49352 * 15\n",
      "Read in testing data set with size of 74659 * 14\n"
     ]
    }
   ],
   "source": [
    "# read in the data \n",
    "# read in the training data set\n",
    "train_set = pd.read_json(r'..\\data\\train.json')\n",
    "test_set = pd.read_json(r'..\\data\\test.json')\n",
    "print(\"Read in training data set with size of {} * {}\".format(train_set.shape[0], train_set.shape[1]))\n",
    "print(\"Read in testing data set with size of {} * {}\".format(test_set.shape[0], test_set.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the pipeline\n",
    "variable_unchanged = Pipeline([('variable_extractor', VariableExtractor(['bedrooms', 'bathrooms', 'price']))])\n",
    "\n",
    "bbratio = Pipeline([\n",
    "        ('room_ratio', RatioCreator('bedrooms', 'bathrooms')),\n",
    "        ('bbratio_dimup', DimOneUp())\n",
    "    ])\n",
    "\n",
    "bpratio = Pipeline([\n",
    "        ('priceroom_ratio', RatioCreator('price', 'bedrooms')),\n",
    "        ('bpratio_dimup', DimOneUp())\n",
    "    ])\n",
    "\n",
    "llcluster = Pipeline([\n",
    "        ('ll_extractor', VariableExtractor(['longitude', 'latitude'])),\n",
    "        ('ll_cluster', LLCluster(init='k-means++', n_clusters=10, batch_size=200, n_init=10, \n",
    "                                 max_no_improvement=10, verbose=0, random_state=random_state)),\n",
    "        ('llcluster_dimup', DimOneUp())                     \n",
    "    ])\n",
    "\n",
    "feature_process = Pipeline([\n",
    "        ('feature_cleanser', FeatureCleanser()),\n",
    "        ('feature_union', FeatureUnion([\n",
    "                    ('feature_counts', Pipeline([\n",
    "                                ('feature_length', VariableLength()),\n",
    "                                ('feature_dimup', DimOneUp())\n",
    "                            ])),\n",
    "                    ('different_features', DiffFeatCounts(random_state=random_state))\n",
    "                ]))\n",
    "    ])\n",
    "\n",
    "photo_length = Pipeline([('photo_extractor', VariableExtractor('photos')),\n",
    "                         ('photo_counts', VariableLength()), \n",
    "                         ('photo_dimup', DimOneUp())\n",
    "    ])\n",
    "\n",
    "description_process = FeatureUnion([\n",
    "        ('description_length', Pipeline([('description_counts', DescriptionWordCounts()),\n",
    "                                     ('dc_dimup', DimOneUp())\n",
    "                                    ])\n",
    "        ), \n",
    "        ('description_tf', DescriptionProcessor())\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "date_process = Pipeline([\n",
    "        ('hour_extractor', DateProcessor(wantyear=False, wantmonth=False, wanthour=True)),\n",
    "        ('hour_dimup', DimOneUp())\n",
    "    ])\n",
    "\n",
    "# categorical variables: manager_id, address and building_id have the similar process \n",
    "building_process = Pipeline([\n",
    "        ('building_extractor', VariableExtractor('building_id')),\n",
    "        ('building_union', FeatureUnion([\n",
    "                    ('building_counts', Pipeline([\n",
    "                                ('building_length', CatVariableCounts()),\n",
    "                                ('building_dimup', DimOneUp())\n",
    "                            ])),\n",
    "                    ('building_indicator', CatVariableIndicator())\n",
    "                ]))\n",
    "    ])\n",
    "\n",
    "manager_process = Pipeline([\n",
    "        ('manager_extractor', VariableExtractor('manager_id')),\n",
    "        ('manager_union', FeatureUnion([\n",
    "                    ('manager_counts', Pipeline([\n",
    "                                ('manager_length', CatVariableCounts()),\n",
    "                                ('manager_dimup', DimOneUp())\n",
    "                            ])),\n",
    "                    ('manager_indicator', CatVariableIndicator())\n",
    "                ]))\n",
    "    ])\n",
    "\n",
    "address_process = Pipeline([\n",
    "        ('address_cleanser', AddressCleanser()),\n",
    "        ('address_union', FeatureUnion([\n",
    "                    ('address_counts', Pipeline([\n",
    "                                ('address_length', CatVariableCounts()),\n",
    "                                ('address_dimup', DimOneUp())\n",
    "                            ])),\n",
    "                    ('address_indicator', CatVariableIndicator())\n",
    "                ]))\n",
    "    ])\n",
    "\n",
    "\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_predictor = RandomForestClassifier(random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_final = Pipeline([\n",
    "        ('Union_variables', FeatureUnion([\n",
    "                    ('unchanged_variables', variable_unchanged), \n",
    "                    ('bb_ratio', bbraio),\n",
    "                    ('bp_ratio', bpraio),\n",
    "                    ('llcluster', llcluster),\n",
    "                    ('feature_vars', feature_process),\n",
    "                    ('photo_length', photo_length),\n",
    "                    ('description_vars', description_process),\n",
    "                    ('hours', date_process),\n",
    "                    ('building', building_process),\n",
    "                    ('manager', manager_process), \n",
    "                    ('address', address_process)\n",
    "                ]))\n",
    "        ('predictor', rf_predictor)\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
